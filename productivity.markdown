---
layout: default
---

<ul>
	<li><a href="#google">Engineering Productivity @Google (Michael Bachman)</a></li>
	<li><a href="#netflix">Productivity Engineering at Netflix</a></li>
	<li><a href="#linkedIn">LinkedIn's Engineering Productivity Tools</a></li>
</ul>

<h3 id="google">Engineering Productivity @Google (Michael Bachman)</h3>

<p>I'm Michael Bachman I'm a senior engineering director at Google I've been at Google for about fourteen and a half years I guess my claim to fame is I was one of the first engineers within the discipline way back in the day it was we had around 300 engineers back then and we were still trying to figure out how to ship code into production without wrecking everything so except we've come a long ways the talk is gonna really center around the discipline at Google and that some of the key things that we did earlier on and then where we're gonna go I believe engineer productivity has been a staple that's allowed us to continue to scale and stayed relevant there's not many 80,000 and 100,000 employee companies that you know of that are nimble fast iterative innovative and commotions react very very quickly in production and you need this discipline to continue to scale and compete within the industry first against smaller companies that can move a lot more quickly and as others that are actually continuing to innovate and whatnot so let me define and you spent a couple slides defining an engineer productivity and then we'll go from there I do want this at the end I'm gonna go through some quick slides but then I do want this to be sort of interactive and I'll be able to answer as many questions as legally obligated ok so here's sort of the the block diagram of sort of what our vision of engineer productivity as those in the back you can't see it but basically it's our job to understand the entire developer workflow from authoring all the way through experimentation production and monitoring and optimize that and remove frustration and toil from that process and so we look at each individual aspect we instrument at all and then you have horizontals that specialize within those different categories to sort of streamline it so how do we how do we do this we have about 2,000 engineers just in this discipline at Google that's focused on making it this isn't just basic developer tools missus developer productivity they are responsible for making developers faster we look at release compression we look at developer compression we look at Bill optimisations and so on we think of the scale that we operate at we are over two billion lines of code that we have to deal with and this number I think I pulled for up a couple years ago so be safe we deal with over 60,000 code submissions every day that's the throughput that we have to deal with and we have a you know our engineers are used to you know sub our sub 30-minute workflows before they check and that changes and if anything impedes them we're not you know optimizing for productivity no matter how often a release is going on in a production which a lot of our core services for at least weekly if not daily automated fully no manual testing that's on average there's some obviously both ends of it and just to sort of resonate in terms of that this is in QA or this isn't test our average engineer to engineer productivity ratio is about 18 to 1 so there isn't we don't try to do QA we don't try to do tests because we're not funded and we're purposely not funded to do that because you get if you scale and you can scale through empowering developers to writing production quality code or earlier on you can get to that point where you can do that cultural shift or the entire team in the entire company owns the quality and of their code and the health of the product you can move faster because you could actually check those things in a certain with the performance and the functional the functional behavior and whatnot earlier on so you could check it and forget about it and automate it and push it into production and then you can react quickly later on if you have the right tools and whatnot set up so as a discipline when we got started in about 2004 and we created sort of three job categories in the beginning the first one which we again if you think in the maturity of this discipline we started with tests and we started in making checking you're in your Co running tests running in the scale as fast and as easy as possible then we went to releases and we do this across three job letters one is what we call studies or software engineers tools and infrastructure they are core to writing the instrumentation the tools and the infrastructure that we have that empowers the dist when you have test engineers that are also technical engineers that do the more the system's thinking break down breaking down complex problems look into gap analysis looking at coverage analysis and really driving and closing those gaps and then you have release engineers in some areas that are needed in terms of separation and duties pushing things in a production automating those workflows or whatnot all right so where we start it off first and again there you have to get to the point where okay you can check in your sails with confidence then you can get your releases compressed and automated and you can go from there what we really started in the beginning was continuous integration continuous development and release automation the entire goal was automated releases automated check-ins weekly releases daily releases and getting to the point where is that far you then got to the point where you had automated what we call canary the term comes from canary in a coal mine where you're actually rolling out the percentage of production and traffic and you have certain assertions and validations that happen to understand if that binary is actually going to be the one that you push ya automatic failover rollback and whatnot so this was the first phases of engine and productivity from probably 2004 to about 2008 and 2010 and then we once we sort of solved this you never really solve it demands continuous we sort of moved on to where we're going and in the future so I think beyond that now we're sort of looking at okay how do you take this more forward this is where we really expanded it if you look at it you know the presubmit workflow if a developer's ready to check-in that change and ok let's say it takes them a week to get that change into production fully globally that's the tail end of what they do a lot of the pain that we've noticed at Google is authoring code complexity integrations dealing with others how do you design it how do you refactor how do you deal with code health you know once they get to the point where they have that CL or that change and they're ready to push it into production that's the last mile so happens so now how does this discipline impact everything else that they have to deal with how do we make IDE smarter how do we make debugging easier how do we make refactoring at scale better how do we deal with experimentation so they can understand what they want to they change and push out into production and those are some of the next disciplines that we're trying to tighten them drive into so I'm going to go into some major themes and again I'm going to blow through these so we can get into that interactive part because I want this to sort of be a little bit more social for me too okay the next piece is we have 2000 engineers how do we operate as a 2,000 person functional discipline well you need to standardize and sort of roll out some best practices across the company and so we've started disciplines that are horizontals in terms of how do you deal with ML validation and skew detection with ml with Emma models that are getting pushed everywhere and dealing with that in production what do you do with mobile in terms have both mobile apps but also the OS and the Android OS and the kernel changes and all of those aspects as a full stack we need to be able to deal with that with the different devices and the hardware software interfaces that we're dealing with we have special special infrastructure that we implement and rolled out and teams across front-ends and backends and pipelines and so it was further taking the individual problems that teams have to deal with and then streamlining it across different horizontal disciplines as well so one of the first things that I want to sort of talk about is we're now looking at ml within the discipline that we deal with so early on at Google it was run every single test every single change and that helped provide okay you did it based on dependencies and other heuristics and I got you a lot of quick coverage but then not only can go so far once you have 40,000 50,000 engineers imagine the amount of resources that you need to run every single test everything change at that type of scale so now within your entire developer workflow if you instrument everything how can you build models to make better ROI based decisions should I run that test should I push that change in in production what other coach I dynamically linked in all these different aspects in terms of understanding how to optimize both the resources and the time that developers spend in device and so now what we're starting to really look at is the ml applications within the discipline next you know what I like to say is Google's trying to continue I'd like this end-to-end seamless experience for our users where your your phone and your browser and your Chromebook and your watch are all seamlessly integrated you can talk to your Google home about it and get all of this what this what happens is behind the scenes all these products now are integrated with each other and you can't get to the point where you could just push one monolithic you know release in the production for all of Google so how do you do smarter unit integration diff flow testing at scale without slowing down the entire sort of wheel that's that's a big challenge that we're actually trying to deal with right now with the tolerance that developers won't deal with anything other than like 30 minute you know end-to-end cycle times and checking in there change when they have to depend on 10,000 different other services very difficult challenging next in terms of watches and phones in the integration between hardware and software that we're starting to see you know we're living in a world where software is moving just from you know from a back-end service to an API to a front-end now it's you know in your car on your watch on your phone in your home in your microwave in wherever else you can think about how do you deal with that hardware software interface where you don't necessarily control the software you necessarily don't control the hardware you're merging it you don't control the kernels or the drivers or all those different aspects how do you handle the complexity of that at scale within a Productivity discipline somewhat off those are also things that are starting challenges as a company alright um next there's also like what happens in production so it's easy to go and streamline you know I was talking to someone at Google and and I in they were like how do you solve productivity at this scale you know and I said well if I want to make developers productive I'd remove everything from the workflow and let just let them check in anything that they want instant productivity increase right like hey guess what no more tests don't worry about anything just check it in there we go productivity solved yeah what about happiness we'll just have them go and like to lunch all day and just check in whatever they wanted then they're good but we have to close the loop with production and what I mean is we have to understand what's happening with our binaries out there the user flows and it's not just about sort of functional QPS and is the binary up in serving and can it fail over and what we deal with that but it's also is the binary behaving where's the service behaving like we expect it to and so you use production and you build assertions within the code itself or within the login itself to understand how it's how it's actually behaving versus it's just functionally up and then so that's on the assertion size as well as sort of closing the loop in terms of instrumentation of metrics and understanding teams might be productive but maybe their systems falling over all the time in production and so maybe we need to figure out what's going on so to that end we are we instrument everything we wanted we know we want to know who's checking in what who's breaking what who's dependent on upon what what they're running what they didn't run what's happened in production what's happening when we roll out a binary what's happening with experimentation so not only can you take a step back and see a holistic view of the productivity of a team and the gaps and the coverage gaps and whatnot but also sort of where they're going and what's how they're trending what's impeding them and then you could actually use this data and feed it back into their workflow to be able to optimize it a little bit and so this is us embarking on sort of a very metrics insights driven workflow as long all right so that said I wanted to blow through the content to set sort of the agenda and the in sort of the the topic of engineer productivity at Google but the now we are sort of want to open it up into a QA piece because we only have about 30 minutes so about 15 min so I'll repeat the question how is the 2000 organization distributed and then how many leadership positions or how is it how is it distributed across a handful of leaders sort of senior design senior engineering leaders and what okay so the so basically around 2008 up into till 2008 we were a central functional discipline then the company went through a change and we broke up into what we would call business units or product and so each product area has its own engineer productivity focused team most of them aligned with a leader either a directors in your doctor or VP and then you have certain teams that have built horizontals that sort of try to tackle it across Google that are funded within some of those product areas given its focus or interests such as Android might be tackling a lot of the mobile infrastructure to make motors what are the key key metrics to measure developer productivity this so we look at let's so we use the term CL which is a change list which is the change that we're checking in to the codebase we look at everything from how long that took to get submitted how how many times it was for submitted if I passed a queue that was run through so we can understand the value of what tests were actually trying to automate we look at frequency of releases we look at time and took to get to cut and push a binary in in production we look at what we call cherry picks or hot fixes into that if some teams do that you look at roll backs we look at Co coverage we look at issues within production in terms of outages or you know pagers or things like that I can continue to go we even look at code review time we look at code do you know basically code late code review latency by person to see if there's certain people that are bogged down in terms of you know focal points within the team maybe they can distribute it better we instrument basically everything from defects to check-ins to releases to what's happening in production the challenging part is that only it gets to a point where an individual only like me can look at the entire view and try to tell the story and so we're also trying to figure out how to make that easier to navigate what things could do to improve the productivity use this basically historical data Ashley's guide your streets so for example you know that based on this data doing this or you would just this too so the question is I think it's more around sort of like agile development in terms of do you use this historical data and to be able to guide sprints we have different teams that if they do thi we have different tools that measure their iteration velocity and then what they can do outside of this max how many roll backs do you have and second is if you take a real-life example of production outage and so I'll answer the first question the question is how many on average how many rollback sort of we see it depends on service and team and maturity and and also their tolerance too so there's teams that are trying to iterate and get a product into production or they're trying to figure out what's gonna stick to the wall sometimes you actually see releases and rollbacks high in those teams and that's fine if they if they can understand and they understand what they're measuring and they're purposely saying hey you know if we want to experiment we want to throw it out we want to see the six-four fine rolling in fact and iterating quickly through that that's okay we don't want to do that for like a hundred million dollar revenue business because you actually want to be a little bit more purposeful and so those ones we actually try to get it down to you know so you know it can range from you might see one every handful of releases being rolled back to one out of once a month or whatnot it really depends on sort of the maturity the team mystically and so then your second question is in terms of dealing with an outage in production what's our role so there's sort of a couple different models at Google there's there's a team called essary that really is the lifeblood a lot of keeping our critical services up at Google they carry the pagers they hand over the pagers they do MacPhail overs and planning and whatnot in those situations one it's I believe it's our responsibility understand what happened and to understand how to close those gaps pre cement and potentially post them it leading up to the release as well and so we like to participate in those we might not necessarily resolve the fire as it goes but there's times where we have enough knowledge of the system where we could go on call and help deal with what's going on - there's other services that don't have us are we hope that we actively play a part and that - I'm gonna go to you and then I'm built testing and deployment of different platforms different applications different languages different code bases you talk in my world okay how do you kind of collect all this data together to a point where you can actually learn from it and build models to be machine learning models are what is your process for bring it all together in a way that you can look at it like holistically all I use data from the disparate sources to make informed decision on productivity a specific area good question so just to repeat it for video and everything else if you have a world that's non-uniform in terms of languages in terms of binaries in terms of what you build and how you compile and how you test and and how you disturb you know how you person push in production how do you bring how do you one instrument and then bring all that data together to be able to use within models or build insights from okay so it starts with building sort of a metrics in insights platform where it has a consistent data model right and a data store that you could push data into and then you could build from and learn from that and so then you actually have to go and build a handful of pipelines that deal with each of those type of situations independently and so if you let's say we have some new JavaScript service that we're trying to build up and compile at launch and this amounted we need some insights from it we might not necessarily have the entire pipelines to measure everything we'll have to build the pipelines to go but to feed into our common data store in our models so then those can train over at least expected data or or at least could be able to compare and feed it into the cut line do you have there so if you have a stunning model that you have is it something internal is it something you can share internal right now but again sort of the reason why I sort of all also wants to sort of participate more and actively in these type of forums is I feel like you know when I when we started entering and productivity back in 2003 2004 we were maybe one of the first companies trying to do something similar because we recognized we had to do it to be able to keep up at the scale that we were growing on but now I think if you look at the room and the amount of companies that are within here I think the disciplines arrived in this continuing to sort of propel the valley forward to high tech forward and keep you know companies iterative fast and this is great for them and the consumers and everything I want to push us to open source a lot more of the innovations that we have and then work is a collective ecosystem to solve some of these things to help teams not to get overloaded by the data and figuring out what what's the first thing that's a great question so the question is how do you visualize the data and get teams to actually address something what matters and understand it I I think it's right now it's more of an art than something that's been solved so there's a handful of certain metrics that we look at which I sort of rattled off that we try to at least have every team looks at in terms of how fast you get released an introduction what's your release frequency what's your rollbacks what you was happening in production in terms of pages and things like that but then we've also started to get teams to look at what you call a like a project health level bar and say okay what would define project health level one and let's say certain criteria certain velocity certain sort of stability defined level 2 3 4 and 5 to get teams some kind of additional target that they could go to and so we've tried to we've rolled that model out into the company to be able to simplify sort of that world as well as give them a target to hit once they reached ok hey I can get basic code into production Wow what's next so I have a question like let's say you have a - and how do you like convince other teams to end up this duel because like you can go like okay here's the tool use it if you want or why here's the money you have to use this pool otherwise again don't get into production how do you find the box yeah that's a good question so the question is how do you find a balance if you're working in this discipline to get a team to use the tools or or do you mandate them to do so I I think mandates have to be very carefully used especially at a company like Google where it's pretty much bottoms up so I think just like the old traditional QA and tests disciplines never like the throw it over the wall model I think our devs don't like the thrown over the mall wall model as well right and I've been in those situations where it's like hey you know what I really think you should use this here you should use it and they say like what is this like how is this it was out my life it's not designed appropriately or it doesn't think about this or that or what not I think you need to talk about their productivity challenges look at the data talk about what's slowing them down and then try to build something or at least agree to build something together and and then sometimes you'll launch it and they'll come sometimes they won't come and you'll rinse and repeat and try something else but I think it's just a natural sort of engineer workflow of understanding what's frustrating them and their toil and what they need and what the priorities is and then go in interest trying to build something that they're involved in the design as well just like we would like to be involved on this I - this is a good question I think in the back and then we'll go over here run tests good question and we do do that so so the inputs it could be and so we do a handful of different things let me tell just for your question in case employment we look at we have models that look at the ROI of a test so if a tests is never failing over a certain period of time it's a small test it's a stable stable period of code should you continue to run in creaseman that's a question right um or you so you can build models in terms of depending upon the change here's everything that you've executed in the past here's the past failure rates of what happened and then you get to under and maybe even how long it took and you can have some function that says okay I want to actually maximize the time I spend testing something and then the odds or the the odds have been actually catching a buck and then you can play with that or give teams the ability to play with that so they could say hey you know what I want to skip tests up to this point because they've always been passing and then what you do is you just push the risk postman and then you if you have a good sort of rotation that watches builds posts in it you can wrap quickly then you get adjust your model or tolerance is there too so so post submit all the tests are on yes yes flaky tests are a different story and how you actually handle those but post them in everything's wrong because it doesn't necessarily it just impedes the people that are dealing with the bill keeping those builds green not necessarily developers trying to check in to change red shirt along the same lines do you do any sort of static analysis based targeted testing anything on that territory yeah so we look at writing so they're repeating the questions all right thank you is um do do any static analysis based sort of testing on the answer is yes you have basically lit checks and things like that around sort of design patterns or whatnot better easy and then you also could do sort of functional assertions that are run statically over the code to look at hey these type of things should never be done within the code please make sure you do this question you're gonna guess so is there anything that your team has either started doing or stopped doing that once you did that or your reaction was like man we should have done this years ago yes so stop doing I so I'll start with the stop doing because I think that's a harder question uh there's there's times where it's easy so engineers are sort of a a limited resource right and I think some of the harder questions is if you have two thousand engineers in the discipline and you have 50,000 and tech or whatnot what problems did you tackle what are the greatest problems how do you make its decisions to stop one thing and move on to something else I think that there's a handful of stories where you go and you continue to sort of work in a particular domain because it was important a couple years ago and you're trying to keep it on and it's very hard to switch into something new I think we see that all the time whether it's a tool and and it's just not being used anymore as it's just being harder to keep up with it and we just haven't made the call on either to rewrite it or deprecated it or whatnot I see those and then something that we wish we did a while ago um I still think mobile developments extremely hard I still think OS developments extremely hard I still think we haven't solved and we're not getting ahead of the software hardware interfaces and how to do that it's scale outside of sort of emulation I wish we sort of solved that I'm living some of those pains right now on the Android side I just feel like sometimes this discipline given sort of we're new and many were maturing we're also tackling the pain points worse once they're also sort of realized versus predicting them do you have any link or do tools different teams yeah good question I think one of the challenges within Google is I've talked about we're trying to standardize and we're trying to build horizontals that are standard I seems sort of those workflows and full sets and whatnot we're striving to do that that the challenge would then you know Google is in is there's this I want to build I want to innovate I want to build my own tool and so there's this always this constant build first by question where it's well they're thing doesn't meet ten percent of my needs meets 90 percent but I'm just gonna go build my own one right and there's also a question on do you depend on someone else or you know inner inner eight quickly within your own teams and just traditional engineering sort of challenges that product faces that we face and whatnot I think we're still trying to strive to get to the point where the developer experience feels like a unit a uniform thought-out experience but we don't necessarily even have any within our discipline Iser so it's engineers trying to build that a I'm like experiences Google is famous for experimentation including you know the famous people analytics groups running a blue tent somewhere to place M&Ms and whatnot how much of that do you incorporate in your tools or are you thinking about maybe expanding that is you know there's a lot of best practices in rigor that's gone into how production software is developed and deployed how much of that translates in your opinion brush into what you're working on so repeat the question so the question is in terms of engineer productivity how much traditional sort of production rigor in terms of experimentation and and whatnot and sort of those insights and those type of workflows do we bring your discipline so if you're dealing if you're looking at impacting sort of the workflow of let's say 40,000 50,000 engineers you have to be really thoughtful about what you push to them and so we will do a lot of experimentation we'll do a lot of holdbacks we'll do roll it out to 1% of this group and let's see in terms of the measurements and the metrics that we we're talking about but see what changes but so the question around let's start skipping tests and see what happens that was a big thing we had a look at in terms of okay are our models right are we gonna start skipping things and move a bunch of pain to our posts amid build coughs and then everyone's not going to be happy and so you go through the traditional sort of experimentation launch look at metrics roll out process and we try to bring some of that rigor to the discipline as well especially if you I mean your respective we go wrong you could slut you could you could stop Google for a day and that's probably not the best thing they'll catch up with me at some points in front of an HR meeting but hopefully not any other questions yes bye probably the last one and then like three roles within the discipline how was that changed as discipline has evolved that's it that's actually a great question and um so the question is these three roles how have these changed since the get disciplined is involved so I think release engineer has been the as a as a role as it's been defined has been fairly similar though the challenges in terms of integrations and building and releasing the code has grown in complexity I think test engineer it was modeled out of the original need of someone looking at a feature way back in the day when Google was trying to actually do testing or like QA or whatnot that's evolved into more of a systems level you know technical engineer that's breaking down adding automation doing insights doing gap analysis and really sort of trying to compress different aspects of it and then one of the largest ones that's continuing to sort of changes our Sethi or software engineer tools and infrastructure role this used to be what called software engineer and test and we renamed it once the discipline once we've sort of took a step back and realized that the discipline was really beyond test and we moved past just that aspect to everything else within the workflow and so we renamed it and we're still trying it's becoming more of this generalized tool and infrastructure role as the discipline continue to expanded all right</p>

<h3 id="netflix">Productivity Engineering at Netflix</h3>

<p>so welcome everybody to the per to be engineering Silicon Valley Meetup my name is Mike Makar and this is Sangeeta Seguin and I have been running this meetup for I think I don't know some variations meetup for a year and a half and so I'm gonna kick it off so we'll start off with a I want to give you an overview of what we're gonna talk about today so I'm gonna kick us off by talking about the history this made up a little bit because I want to give some context for those who have been to a previous incarnation of the meetup and where we're going and then sangita's gonna share this our ideas on productivity engineering I'm gonna talk a little bit about productivity engineering at Netflix and then we're lucky enough to have pan here from Strava who's gonna also talk about productivity engineering and straw and so will hopefully get through this quickly but effectively so productivity engineering Silicon Valley redos what does that mean so early last year Sangeeta and I who we both work at Netflix I manage the Netflix developer productivity team Sangeeta manages the edge developer experience team both of our teams do very similar things and we got to talking about kind of the parallels between our teams the challenges we have um the work that we do and some of the innovative solutions we've come up with and we've you get looking around the industry for a conferences and meetups and trying to see who else is talking about some similar things and I think we were a little surprised to see there wasn't as much conversation as we would have liked around some of the challenges we have and so it got us thinking well maybe you know there's something that we need to start we build a community around some of this work and we didn't we started thinking about what we do with the community and we decided starting a meetup would be the next best thing and we started the developer experience Silicon Valley and this was probably last year and things went well we had a great lineup of speakers we would do her we're doing a talk every other month with two speakers we had who talks from LinkedIn yet talks from PayPal Coursera and of course Netflix and you know we we had great attendance we even had we've called in the videos like I did just talked about recording and got you everyone getting Nick gave me consent so we were doing pretty well and and I think we were excited about the Meetup and the progress he's made by the way if you're interested you can see all the previous talks on YouTube they're still posted but something was missing so as syncing and I started exploring this we realized we had certain challenges as organizers meetup trying to figure out what fit into our meetup and what didn't so when we talked to people about what they want to talk about we said well you know developer experience is the focus but what does that really mean and you know it's kind of not like testing but not really like testing and kind of like DevOps and so we had trouble really self selecting or selecting what would be a great candidate for the meetup I realize that was a problem that we needed to address and so we decided to strata then continue to have meetups we took a break from the meetup the loop and really started thinking about what we were focusing on what the problem had originally drawn us to start a meet-up and we still had some questions so we if we look at our meetups like Sangeeta's Sangeeta's team is the developer experience team so what is developer experience mean what what about developer productivity that's my team name so what do we mean by developer productivity what about non developers so we have I know that some people maybe some people in this room have given me a hard time about the fact that use developer in my team name what about all the non developers that were companies and what isn't this just DevOps or isn't this just agile so these were questions we challenged ourselves with around productivity engineering and as we continue to talk to not only our peers at Netflix but also peers in the industry at other companies we realize we we took our time really coming to the conclusion we really took our time and for those of you who have tended I think the last incarnation of this Meetup it's been a while but we think it was worth it we posted a blog posts on medium I don't know how many how many people have taken minute to read this ok if you haven't I'll send make sure that send out the link so this was this blog post and sikita will spend some time talking about their ideas but we feel like this writing us putting us together and doing something doing some research helped us understand what we're really talking about and as we went through this we realized we want really weren't really focused on developer experience but really this idea of productivity engineering lower back organizations and as a result of changing are focusing on productivity engineering we really decided we needed to rebrand the meetup so that's why we're here talking about at the protein engineering Silicon Valley meet up and not developer experience cuz that for those of you who've been to the meet before I wanted to get some context about kind of where we've been and and and how we got here she still might be asking yourself what is productivity engineering so I'll let Sangeeta take over for a little bit ok imagine yourself on a weeknight you had a long day at work went home had dinner start to wind down you'd turn on TV and you turn on Netflix and you're wat wanna watch and a lot of people watching these days stranger things this really happened it happened this was a bad couple months ago and what had happened was an engineer made a change they thought it was simple it was it was someone asked him to do something it wasn't that hard they thought it was safe but it wasn't it ended up black holding a bunch of traffic and this is what a bunch of customers saw in in a subset of our customers so now of course we have all sorts of things like the ability to fail traffic over and all that which helps us but did this just happen every once in a while so we did an incident review lots of actionable learnings they're all evolving implemented by now and I'm pretty confident that this particular problems in might happen again right so great but you know every once in while this happens and I'm most of the time it isn't something that you at home as customers see we take care of that behind the scenes but you know what's a lot something like this happens but everything does involves someone looking at it trying to process information and maybe taking an action so which I thinking about okay you know how can we make this this better right let's take a step back and see how how we can improve things here and so we went coming back to basics for think about Netflix we have we follow agile devops have a culminating architecture and we've festive resiliency fault tolerance all of that we're still you know why does it take effort and what can we do me so taking a step back really what's the goal of business we're all working in organizations working you know working hard and we leave to deliver business value that's our number one goal and so practices like agile devops services so really nice tools automation culture all these things are great they're the help us optimize doors getting your satisfying our core mission which is to deliver business value now man as in physics and in real life there are also factors dragging us down why it's a contract and these take some some combination of these factors right there's complexity there's complexity in terms of you are always adding new features or you dealing with a myriad of versions right how many Android versions are and you know used to be you have to worry about you have two flavors of mobile devices and browsers and now you have to worry about your toaster and your fridge so you know complexities crawl scale now scale could be traffic scale could be the number of developers hopefully there's a correlation between the two and availability and one of the things that as we we see this we think about is availability is in isn't just for companies like Netflix or Facebook or Apple or Google all right availability you know we've all become it's kind of like those two year olds who need whatever now and so imagine you know pan you're gonna talk about stralla and you know I take my phone alright decided out for a run or a rioter and I won the app working now right it's not it's not cool if it's time for 20 minutes or is it the availability requirements and expectations have gone up at the same time there's a need to move fast you have speed wins it doesn't matter what size you are you were the way of Netflix you know Google you know you want to traffic to more fast so you know there's this this tension there and there are all these requirements that were trying to fulfill so going back to the tools we have today we follow these practices we still different culture we have those technology automation and doing a pretty good job we're getting there we were seeking by just the added cognitive load goes up so these are cycles you're spending worrying about how to scale your software how to deploy safely how to move fast and these are cycles and we'd much rather be spending towards actually making yourself or better so in effect what we want to do is try to do streamline this process right try to change the shape of that a little bit so here you're trying to as I said earlier maximize the top of the energy spent towards offering business style so it's within that framework then that's really what we think productivity engineering is it's it's this notion of reducing cognitive load and we want to spend the bulk of our time and energy storing business value there's no statement in here about who does it do you build solutions that you don't do you buy them outside I think there's a whole point here is if somebody is spending the time focused on this and really that's that makes it effective makes the rest of the organization more effective that's another framework to think about this and I'm sorry sort of move into this area right right how much you know maybe you're a small company and you're doing some of this already maybe you're Netflix and you know we have a lot of these things thanks already right but what's the effort that's going in to achieve the results you want and imagine if you could get the same results with less effort so that's what would you do with the energy that you're saving so that's really where we think you know Mike in my team's we've spent a lot of our energies trying off to my exactly and so within that we've seen a few patterns are these are obviously are influenced a lot by what we see here within Netflix but we have spoken with some folks outside in the community and seen that so what these resonate and what it loves to do is have a dialogue afterwards with all of you and health you know have you helped define this change it let us know what you think about it so three patterns that we see are well for experienced platforms and this idea of centralized enablement I'll run through these real quick comes some key themes here and then Mike's gonna follow up with examples of how we do each of these at networks so developer experience the core tenets here in my mind is this idea of treating internal tools as products right and that means everything from UX or understanding are you building the right solutions your tools should fit into developer workflows the ergonomics as infants if someone's gotta go out of their way to use something I'm not gonna use it effective your empathy for users you know my own experience there's a different mindset if your server engineer it's different points or if you're you know a web developer or a JavaScript engineer what's the cause your target audience and tailoring your tools to that and you know I like this refined annoyance it's a term I stole from a podcast I heard from Jason yep of fuck works and this this idea of not settling you know all of us have been in this scenario where it's ten clicks to get to this page or I got a log in there and then I go here and you know well that's the way it is or it takes 30 seconds to load a page or whatever I got there and I got my top 10 but over time these things start adding up and you wouldn't go back to Netflix you'd if it took a minute to load every time or you know heels rebuffering so that's what we should expect and not settle and I think both if as tools providers and as consumers these are some some ideas to keep in mind really how quickly can you get developers productive so I'll share this this metric that I found interesting it's called TT f h w anyone want to guess what lips it's from the API space-time - first hello world right how quickly can you discover a PS get your Wolski's whatever it is that you need so those are some ideas there it's kind of hand-in-hand with developer experience or maybe foundation for developers this is to say you have platforms we follow it a lot at Netflix it finds us a value and some core tenets your abstractions right I don't need to know how service discovery works if my job is to work on recommendations I don't need to know what version the compiler I'm using are you upgrading credit or whatever is extensibility there is no one-size-fits-all we know that and so while we try to target the 80% use case it's very important to make sure or not you know we're leaving ourselves open to extensibility and api's cannot stress that enough and we have some great examples of you know you of course you build going back to my previous point great UX allow users to you give them what they need but you'll never get everybody so multiple ways to access the underlying functionality by using api's and all that it entails right if they didn't you manage your versions backward compatibility and really putting thought into that and that's a good perfect and then the last pattern we see is this notion of centralized enablement they could take the form of centralized teams so here Mike's team and a centralized team Arizona centralized team that provides similar functionality for for teams to do self-service failure injection testing and things like that chaos engineering you know some organizations have DevOps teams or it could just be someone on your team was was Rallis just gonna focus on me these are some things we patterns we see and you know the thing about it's just still still lots of questions here right what does productivity engineering do to you all in your organization does it make sense for a small company is this only for companies that have over a thousand engineers I mean if I'm trying to make sure if it's you know whether I want to meet my next milestone and whether I do or not is an existential question right it should have been investing in this what role does culture play and I think a big one is how you define success another thing I didn't put in here was a lot of in my mind operational sort of challenges that how do you get people to start thinking about the operational assistance when they're developing this sooner you can do them so there are some questions lots to answer we hope you'll help us define that and I think Mike is now going to talk give you some more examples of so the concepts that I so so you need a great job of explaining the core concepts of productivity engineering I'm gonna try to map this to our experiences at Netflix and and I'm we're still exploring this space and so we're still trying to define what this looks like and so like to Geetha said please give us feedback and insight and then tell us what things resonate and what things seem absolutely foreign or wrong so going back to this definition that we've come up with what protein engineering is I think the key part of this is really about reducing the cognitive load for engineers or for employees in your organization and so the question we have to ask is what is what increases cognitive load for engineers at Netflix so before we get to that point it's important to understand what helps drive decisions for engineers and for teams and Netflix Netflix is C I mentioned this when we talked about DevOps has a famous high trust right and so a big reason why I wanted to come to Netflix was because not only the technology in the mission but also our culture but what's interesting and why I'm bringing culture up here now is that what I was surprised by is how often culture Netflix ends up being a reason for why we can or cannot implement a technical solution so it's explored like what our culture looks like for those who aren't familiar I think it can be easily summed up in three score statements which is freedom and responsibility which means we give you no engineers at Netflix a lot of freedom and we expect them to also take on a lot responsibility I'm also said differently those with responsibilities have the freedom to make a decision and so that's a good kind of metric I our method I use to understand who who actually has the freedom to make a decision teams at Netflix are highly aligned and loosely coupled which means that we're all driving to the same mission which is making Netflix the greatest you know entertainment streaming service possible but we still have this loose coupling that allows my team to operate with some level of autonomy and not be coupled and and feel like I have to be work working in lockstep with every other team at Netflix and have some autonomy in the space that I'm operating in and then there's also this third concept of context not control which means that Netflix leadership isn't about giving us micromanagement restrict direction but it's about saying giving us high-level guidance and then expecting and trusting that the engineers and individuals at efflux do the right thing so all this this culture creates great opportunities for us and the reason why we have this but it also creates and challenges one of the challenges when I mentioned talking about solutions is because of this culture this high trust culture we often have to lean on solutions that are more more about guardrails and less about gates so if I were to say that we are nervous about engineers pushing to a certain region at a certain time of day a typical answer for inhalation might be let's stop them from doing that and let's let's put a barrier up for them to do that Netflix's approach generally because we we instill so much trust in engineers as we expect that they know what they're doing but at the same time we want to make sure that they have the right information at the right time to make the right decision and so this idea of when we implement solutions inside of tools like spinnaker for instance which I'm gonna talk about a little later it tends a bias towards putting up more guardrails and less about stopping people from doing the wrong thing in because we expect that they have the right information at the right time because they're closer to that problem so looking at this the model of what we think we see contributing to cognitive load there are these four things complexity at scale trying to maintain availability and velocity these things are at play and and as you start tweaking one of them and you're not if you're not paying attention you could create an environment that puts significant cognitive load on engineers so let's walk let's walk through a couple examples I'm not gonna talk about every single example we could find at Netflix of where cognitive load is high but I think I want to pull out probably about three or four different examples I think will really help illustrate what we're talking about and and how culture plays into this tool proliferation is one of them so when I first started Netflix the bootcamp for engineers was about the lab portion was about two hours and in that two hours you were creating a project in a get repo you were creating Jenkins jobs that associated with that you're creating application and at the time was asgard and you were deploying this application it took about two hours to do all that work it was very manual and if there's a whole bunch of errors but and we had to spend a lot of time on the documentation a lot of this was because we we had a whole bunch of tools that weren't necessarily tightly connected or learned how to speak together another challenge is is that we have all these loosely coupled teams that are building all these solutions and they see a problem right then and there they do the right thing and solve the problem for themselves they say hey I saw this problem hey Lauren you want to try this and Lauren says that's great next thing you know we have this potentially alternative but competing maybe even better solution out there and there's maybe three or four of them you have this concept of tools being proliferated throughout the organization and because we don't want to constrain you know we don't want to sit there and say no don't do that it's really about pretending the right context is like is that the right thing for you to do but even with that context we still have this proliferation this problem of lots of tools out there for new engineers it can be daunting to think about which tool to use we have another interesting challenge lifecycle management which you know can what we mean by this and lifecycle management is big enough that can mean almost anything you want it to me but what I mean in this context is when I create something what happens that thing over the lifecycle of that thing great example is these boot camp labs for the longest time we had nothing that was going on back and cleaning up the boot camp lab assets and so people were creating these get repos and Jenkins jobs and applications and they were just sitting there that's a small instance of the problem we found an easy way to solve that problem but you can think about how we want to solve a different problem which is I want it make it super easy going back to our lab example I wanna make it super easy for somebody to do the creation of their application I want to get was a TT what was it th f HW I wanna I want the smallest TT fhw at many things and so this is a real example like we we built generators and applications that will allow me if with a couple of commands to get an application up and running with Jenkins jobs and get repo with all the right paved road tools in place in you know three minutes that's great so that we've reduced the drag for creating that application but now we've created a different problem which is there's a hockey stick of git repos being created all over the place right and they're not being cleaned up and so how do you manage that costs it's not maybe hitting you now but over time it will say get alluded to this idea of operational experience or this OP is the operational cost so how do we make it easy for an engineer who's wearing a pager for their application they get paged into the life how do they get the right metrics on their application to know what is doing how do I get the right logs what is that experience like as an operator of your service and how do you reduce the cognitive load that you we put on operators first for services just another area where we're really good at making make it easy to put things out there we're still trying to make it better for teams to to operate their services and I think sangita's teams doing a great job of investing in this practice of operational experience an area that my team has been working on is this idea of organizational wide integration so what I mean by that is effectively sharing code so how many people have written a library in their organization and shared it with another engineer or team how many people know how many customers of that library there are an authorization how many can easily manage the API is that everyone's using and understand who the owners are and be able to refactor on a whim it's hard it's a really hard problem so this idea that I can easily share code I can I can run a Gradle build produce a jar put it in our artifact repository I start talking to everyone in this room and it's very easy for everyone to start picking us up and using it what happens when we the organization do doubles in size or goes up 10 times and everyone's using that and I need to make a critical API fix or security fix a security vulnerability how do I do all those things how do I know who's using it who's using that line of code if I want to change the line of code who's who am i impacting these are problems that at small scale if this is the size of organization that's a very tenable problem to handle with just like hey everybody I need to change the API who's using it great all right cool let's go over here let's let's change it at a large scale you I didn't know who my customers are and this is a real problem that happens to engineers at Netflix all the time which is they don't even know they think they know I may have told Sangeeta that he use my library but she's told everyone else in this room and I don't know who my customers are these are all I think you know exemplary examples of some of the challenges we have I think that it's produced that that increases the cognitive load of engineers at Netflix and likely some of your organization's as well so how do we make Netflix engineering more productive so going back to these practices at that's an Edith laid out we can we can kind of start talking about how developer experience platforms and centralized enablement are being employed right now to kind of address this high cognitive load loaded high cognitive load of Netflix so let's talk about developer experience so this is a picture of spinnaker does anyone seen spinnaker before I mean using spinnaker right now no Netflix hands go up so Spinnaker's our cloud infrastructure management and delivery platform it is our it is the evolution of probably many many tools and any meaning learnings around how we have built and deployed services into the cloud specifically AWS and what we're looking at is the typical UI for for looking at your clusters which various instances you have in the cloud in this application each of those green triplets is one instance of AWS and so you can see how so spinnaker is being used by almost every single application developer Netflix to deploy to the cloud why is this why am I talking about developer experience here well the reason I think spinnaker is a great example for developer experience is highlighting a point that Sangeeta made earlier it's an internal tool but it's really built like a commercial product the UI of spinnaker really could stand up and kink out almost any commercial product out there and it's it's a very robust tool the team paid a heavy focus on design and user experience they put on a lot of effort in understanding the pains of previous solutions they built on the lessons learned from the team that ran Asgard which is a previous cloud deployment tool and then even Sangeeta steam spearheaded a continuous delivery effort to build on top of the lessons learned from mass guard that fed into spinnaker so a lot of time spent on how do you Netflix engineers really need to deploy and manage their cloud and since their cloud serve is focused on not only UI there also api's so this idea that we want to give flexibility to engineering teams to use the API just as much as the UI so spinnaker and a lot of teams will actually lean heavily on these api's to manage their deployments their their pipelines and applications building on another thing that Sangeeta mentioned which is this idea of extensibility so Netflix is in AWS and we have no plans of moving out of AWS so you could argue that when you're right starting spinnaker you could you could basically build it to just default to AWS but the team had enough foresight to just kind of say well we'll make it extensible the idea of a cloud provider built into spinnaker which worked out to our benefit when there was a fledgling project at Netflix called Titus which to build a internal container cloud Netflix this allowed this extensibility built in a spinnaker allowed the Netflix team spinnaker team to add Titus as a container cloud so now when Netflix engineers could choose if their deployment was gonna go to Titus or to AWS and then I think the last point we make here about spinnaker is this idea of abstraction of the focus niche platforms and what I mean by that is I added this we go back to this picture you're not only looking at spinnaker and your application what you're also looking at is is linking into other systems other platforms other tools at Netflix that serve a very niche specific purpose for instance you can see right here I see that there's a link to eat for this particular version of our application that there's a link to a build that will lead you to the Jacobs job which will help you get insight into the build aspect of this instance there are you can access the telemetry platform called Atlas at Netflix from your spinnaker application you can figure out the how the Canaries are doing which is a whole other system called ACA you can tap into Laurence teams or we're just gonna pick on you all day Lawrence T and he works on the chaos engineering team a tool called chap Chapas integrated in a spinnaker and so this idea that spinnaker not only serves a purpose which is delivery to the cloud infrastructure management but it becomes a subtraction for engine other tools and other platforms at Netflix that really helps reduce that cognitive load we can point engineers and say use spinnaker and spinnaker will help you discover all the other tools that you need to do your job I think that's that really makes spinnaker a great example of developer experience I think another interesting example I wanted to highlight is this other tool called skipper so Sangeeta's team is the developer experience team is part of edge and edges is a sub organization within Netflix and their org and her team recognized that they needed to build something more refined more focused to serve it a slightly different purpose than the typical Netflix engineering experience and so they also built a tool called skipper that sits on top of spinnaker and a bunch of other tools that are applicable predominantly for the edge engineering team so this idea that even we're you know our organization my organization engineering tools are or were building tools for all engineers sub organizations are building more and find tools on top of that to help improve the developer experience in a more localized fashion so sorry about platforms so when we talk about platforms I often think about things that are of course are applicable to me which are things like the build automation platform at Netflix my team is responsible for helping build not just tools but a suite of tools that can be composed together to form a whole platform for engineers at Netflix to help build spinnaker is an example of a cloud delivery platform and instruction management platform there's our PC platforms at Netflix telemetry platforms persistence platforms so when you think about building an application at Netflix or thinking about onboarding these verticals these these different platforms are really the things that you need to learn or you need to get good at or your service needs to integrate with to - to build a service in Netflix but it gets more complicated than that especially for my team because this vertical represents only a stack of platforms for one language as we start thinking about more languages this picture gets more complicated Netflix has traditionally been a Java shop we've built a lot of services and tools and frameworks for the JVM and for java application developers my team going back to build is responsible for building a Gradle base build tool called nebula that builds on that allows people to build and manage java applications but as Netflix becomes more polyglot as we start moving into the world of nodejs Python and Ruby we have to think differently about this this picture here shows how at some point in time our capabilities in these different platforms were very mayson's and how we we had this very robust see all greens you can go to production with Java but not with other languages so for my team we think about both we can see these smaller platforms the vertical platforms how do we address this problem and so for us instead of building reef relearning the lessons the failures or the lessons of building a single tool to solve a single solution for single language we started thinking differently about how we solve this problem so from my perspective we built this tool called newt newt is a commit yeah it's adorable I know newt is a command line tool written and go but the idea behind nude is to stop thinking about a language specific implementation to start thinking about language agnostic solutions and workflow so newt really focuses on that middle area of how teams can not necessarily build their application but how they can integrate their code package their code and in deployed our code into the Netflix ecosystem we decided to take a very different approach instead of focusing on build to focus on really workflow and integration so with newt it will not care about which build tool you're using for job script but it will care about build it will have a new build phase and you attach to it a command that you wanted to run and the newt will help do a whole bunch of in consistency integrations across all your environments such as continuous integration and local environment and your packaging you could tell it my nodejs application is depended on what's a great version of npm I don't know I mean I don't do know jazz development but let's imagine that you want a specific version of node NPM it will make sure that that versions on every single developers machine that's running that that build on the CI server that's running that's yeah build and when you package it up it's going to be packaged up that same version so this idea of providing consistency across environment is more important than providing a singular Rover for engineers and the noot also becomes our developer engineer developer platform for an engineer's and so we can can now start scanning this horizontally across all languages I don't have up here but we have some representatives from the studio team and Netflix here which are building applications in Ruby and Ruby is is one of those pillars that we are adding adoption and integration to for mute ok and so this last part of is centralized enablement and I think this is probably the easiest to address which is for all these platforms for all these different solutions what you really have is you have teams dedicated teams focused on solving these problems and these teams you can consider a really investments that the company is making in addressing this problem holistically once again I pick on Lauren and not because I don't like warm because I think Lauren here works on the coolest team name at Netflix which is the traffic and chaos team but you know the teams in red here engineering tools my team developer productivity is part of engineering tools spinnaker was built by engineering tools but our Charter is to build tools that help engineers get their code out the door sangita's team in red here is also the edge developer experience team and so this idea of we have all these centralized teams focused on solving these problems in a very essential way I think represents the causation investment in these problems but it doesn't have to take the same doesn't have to be about all about teams Netflix is scale we can afford to have teams who are focused on these different solving these problems but you know at some other teams scale it might be a different it might be a person it might be a portion of someone's time focusing on solving these problems around developer experience or productivity engineering and so before I turn this over to Japan and to talk about Strava I want to talk a little bit about the Meetup before we go on so and I'll have time for questions at the end so we're gonna continue doing this Meetup every other month we'd like the model I think if not doing it monthly but doing a kind of a higher value higher impact meetup every other month and alternating um that works for us I'm about to break that rule in a second um we also like the idea of having two speakers so rather than having a lot of meetups I used to run a meet-up in back in DC where I was from we would just do one speaker I liked the idea of having two speakers it kind of increases the value proposition for engineers or for attendees and what we're gonna try to do differently is is try to alternate locations so every single instance the beam up has been held here in Los Gatos so we're gonna try to move it up the peninsula move up to San Francisco so we're gonna make an effort to go back and forth and try different locations I expect I'm making easier on people getting down there or us going up there and we'll continue to record sessions and we'll continue to have the consent yes as a recording sessions but also what would the change here now that we had this blog I would love that I love the idea of us being able to also augment and follow up sessions with blog posts about what we learned or what people talked about I think that could also be valuable so in January next month so we're not skipping January the people at Airbnb have been nice enough to volunteer their space to host the January meetup so we are hopefully gonna plan it for late January well how to to meet up to Tufts two speakers in in the city so that's exciting and if you're interested in telling a story come join us and talk about this stuff this is interesting so questions boot camp is just onboarding he can think about diverse new developers come to Netflix how do we get a Netflix engineer up and running and effective as soon as possible so right now boot camp is two days so yeah so another quick here is that Netflix we hire senior software engineer is only so generally speaking you don't like it doesn't mean that somebody comes out of boot camp two days later and is like thinking about your stock and then deciding which portions of it you could share tooling across tool chains do you have challenges with the bill host OS I noticed it looks like all the tool change you use seemed likely be happy everywhere except windows I mean I don't know you guys a Mac house or EULA we're shocked I mean there's like a 90% jack like our devs are all over the voice I'm from TiVo by the way so we're partner stuff but like we definitely have challenges yeah Oh tooling an efficiency slide where we get something more great and then he's like well I don't want to use it back yeah Linux person why is this suck so much yeah it's like when you chose the sucky path like so deep do you do you live this challenge at all yes and so the majority we saw the Mac case usually first when it comes to like laptop effectiveness but right now we are looking at like I know we have some engineers that are on Microsoft and need to be on Microsoft forbears through her windows yeah so we do have to solve that but it's yeah it doesn't come without pain develop a design in quotes in front of debugging right you can be caught cold reloading and all of that and so that's the type of language you'll never know so we're focused for this case on node and so we have that for the Mac and you know where do you think doctor so Linux VT new and then now when it comes to Windows one on one area we're thinking of investing in is called based experience right so that helps in this case developments and in you know anyway we lambda and all of that that that's an area of exploration anyway but I can see it being powerful and beneficial in health I think was also was missing from the slightest context that like each of those verticals also represents different types of use cases so um the job of vertical is streaming critical services of Netflix the Python vertical is Mitch weird services sometimes written by the security team some but a vast majority are actually data science then you have JavaScript which is like a whole there is a whole team that's focused on a nodejs platform and so there are still there's he doesn't have to be new doesn't have to represent every single possible use case we can be focused on the team specific use case and so you end up having is actually not only just a language platform but actually for that there's like a little bit of tension there where you know if you are building really high level abstractions you know you end up in a situation where you - if they encounter issues that they're sort of their hands are tied yeah I mean I whenever you say something like abstractions like what I think is like people you're not doing this we're not using anything but like what often happens people say well that's an extreme source abstract there talk about extracting everything versus abstract nothing and the answer is really this nuanced middle hard to find exactly where that is but that's the interesting challenge like you've you're gonna find that right tension where I'm gonna provide you sufficient abstractions I think when we think about abstractions at least my team talks about it is its abstract away the common use case so like the majority people don't have to worry about the details but you always want to give somebody the opportunity to open up the hood and actually dig in there and actually do what they need to do I think that satisfies the majority of Netflix engineers needs this as long as I have a way of actually changing this totally from discharge it's hard to get it right right magic is great as long as it works and then when it doesn't work everyone just finding that so what principles we follow is this idea that it's a wax design called progressive disclosure you start with a minimum right and then as needed you expose more installs more and that's actually great example my hat yet spinnaker and all our servant leaders use occur but part of what my team is billing is their jobs to Canada they're not hard for their surrendering skill for them there's even an abstraction level on the spinnaker they don't need to know the spinnaker there are two reasons a clusters and service so they can't drop down to spin' a little thing but they don't have to and that's where API is also help backwards are well-defined API we can build on top of it any other questions okay so give us a minute while we switch out for Penn</p>

<h3 id="linkedIn">LinkedIn's Engineering Productivity Tools</h3>

<p>hello good evening and welcome to LinkedIn second engineering productivity tools meetup if you are here for the first one it was a great success I hope this one goes as well as the last one and we have a set of great speakers lineup today so let me first introduce myself my name is Jenny Cal I am responsible for presentation infrastructure here in LinkedIn my team is responsible for the web framework the API framework as well as accessibility and translation here I LinkedIn I work closely with the tools team the developer productivity team I'm here to help them introduce all the cool speakers we have lined up today so before I get started here are some housekeeping items first bathroom is out the door to your lab is a big old sign you can't miss it also if at any point doing the whole show you have any questions if you're wondering howling didn't do certain things a find one of the people with a blue badge or wearing a LinkedIn t-shirt right ask them these questions introduce yourself network get to know each other we're friends last thing is that there is a booster over there the LinkedIn is providing complimentary professional headshots look at mine I got that done so you can actually get one put on your LinkedIn profile looking all pro so feel free to take advantage of that also the there will be a Q&A session at the end after all three speaker presenters so hold your questions till then we'll have all the speaker here up on stage and you can ask them all the questions cool all right so why developer productivity so a LinkedIn our mission here is to connect the world's professionals and help make them more productive and successful we recognize that be in order to achieve this mission we have to be productive ourselves but common sense right if we're not productive how can we help other people and also this is a fun fact well not so fun if you're a hiring manager or Omnitech company but engineering is a single biggest business investment causing any technology company right it's not it's a fancy expensive buildings now the data centers not a free food not the crazy parts you know it's the engineering salary that's our biggest business investment so if you can help these engineers be productive be efficient operate fast you know do their jobs then you're not really protecting your biggest investment so something about myself recently as I'm not a very handy person right recently I was introduced gently' by my partner to power drills when I was trying to install a curtain with screwdrivers right so it took me 30 minutes to install each curtain but with the power drill I could do it in two minutes wow that was a life-changing experience right so the point of my story here is like we here in this room are in tools of infrastructure productivity functions and it's really our job to ensure that developers are empowered with power drills and now you know suffering and trying to accomplish their goal with a screwdriver all right okay so here I linked in engineering productivity is a high priority we have teams that are dedicated to it we have initiatives that work on improving engineers productivity happiness in general and here today the goal of this event is to share some of our findings and learnings and also to learn from you the industry experts that has been thinking about this problem and trying to figure out how to improve your organization's engineers productivity so I'm here to basically introduce the speakers and here is the first one just me in my notes it's Nadal cohan he he's here to present you it the next generation of developer tool tooling and Netflix Nadav currently leads a new team at netflix focusing on improving developer productivity and velocity and he previously worked at the Netflix including productivity tools related to build dependency inside collaboration and documentation so please join me to welcome the dog to the stage [Applause] thank you all right so thank you all for coming and thank you for that kind introduction I hope you're enjoying the food it's amazing all right and let's begin okay so today we're gonna talk about a tool we built at Netflix called mute or a Netflix workflow toolkit it's something that we built in order to support our polyglot efforts within the company but we do this while enabling our developers you know not to worry so much about the internal plumbing work that needs to be done as part of that development so that means stuff like initializing new project packaging your stuff into deployable units publishing to an artifact repository that sort of stuff we just gonna make disappear as part of the tool the cute little mascot in there that you see is our logo we have stickers of that thing so please feel free to come by and grab some after that also ask questions if you have them but this is probably more important okay so before I start talking about Newt and why we built it and how we built it I think it's really important to explain where we came from what state Netflix was in before we came up with Newt and what what reasons we had to actually come up with it it will give you a good context and a good understanding I think of our thinking at the time so back in the 90s when you know they founded Netflix more or less we had a lot of cool things we had friends right that was kind of successful back then I think it's actually still successful surprisingly but people really love that show we had this thing right that also who was very successful it was kind of revolutionary Haven at the time I don't know and then we had this thing it was like some small company that was basically sending out DVDs to people through mail so they can watch it on their DVD player at home right it sounds maybe sounds a little bit absurd but you know that's the way Netflix started it still exists today but you know in the more limited capacity but it still exists and as you can imagine this this sort of you know endeavor didn't really require a lot of sophisticated technology right back then we didn't really talk about the cloud we didn't talk about micro services definitely didn't talk about polyglot it was effectively just a single big Java based monolith right that was using an Oracle database backed in a data center right that was that was the thing we had back then it worked great right for what we needed to do it worked superbly and for us as a development tools team you know it made sense to use the what we call the native tooling that went along with the JVM right so that was what developer is expected that was the easy thing to do so we started off with ents I don't know if anyone uses and still yeah sorry to hear and then we moved on to Gradle which is the de-facto tool nowadays okay and and as time went by we built more and more infrastructure that was part of that process stuff like you know code for building so you know what's the project what's the project structure and how does it end up how do you test this thing all sorts of metadata that we add in the top of it like what kind of dependencies you resolved or what operating system you built this on all sorts of metadata that really made sense for the entire organization we talked about packaging so we take that artifact we turn it into a package you know we didn't want everyone to do it themselves then publishing to our artifact repository and finally and maybe very importantly we had metrics for everything right we wanted to measure engage all the things that people were doing so that we can improve upon that if you don't have the numbers obviously we can't really do much we can guess so over time I don't know maybe some of you have heard this before all the tools matured into what we now call nebula which stands for an f6 build language it's a set of Gradle plugins that are mostly open-source that effectively enable all the things that I just talked about you can get it on github if you want to if you're using Gradle and that thing is useful to you I in fact we were using Gradle so much that you know we get to a point that we had to hack the actual Gradle distribution at one point because Gradle core was not good enough for us so we had the distribution and we start distributing that internally at Netflix it worked right as long as we were on the JVM mostly and you know we didn't really have to venture into other territories that was fine it worked really well but then obviously developer needs started to change as Netflix grew more and more complex so did the tooling and people started asking for new technologies people wanted to use node people wanted to use Ruby for the quick iterative development for you eyes right and you know we with Java tooling we could support it to an extent but it wasn't it wasn't ideal right they just wanted to use more and here's a chart that shows language used at Netflix over time this is up until 2016 it's not super updated but I think it shows the point you see that initially we started with mostly JVM and nothing else and then over time we grew I don't know if you can see it here but the green the top Green Line is JVM work so it grew but so did other languages the second line there is a java script and then we have other stuff like python and ruby and and more and more and more languages started popping up so obviously Java was still important but it wasn't the only one anymore it wasn't the only and looking at it from a different angle you see like relatively speaking that node or javascript is actually almost as big as Java right it's something that happens kind of underneath the radar no one really asked for it explicitly just kind of happened and we've been hearing more and more chatter about it and so you see we have a lot of different technologies here we have JavaScript which is a combination of our complex UIs and also back and they know do we have stuff using Ruby and Ruby on Rails for UI development or fast prototyping rather and then python for machine learning and data science right so everyone have their own kind of tool so obviously developers still needed to package stuff and publish stuff but they still wanted to use those languages that was the you know and they didn't have a built-in solution other than nebula so you know developers being developers did they want to reinvent the wheel so obviously what they did is they hacked around it right they they found solutions that works for them and I want to go through a couple of solutions that we have seen at the company you know before we were able to address it and I'm sure some of you could relate to to what I'm about to describe so the first polyglot hack that we've seen has to do with building node apps okay so in this case what people basically did was they completely disregarded their tooling they said all right I'm gonna install no than NPM locally on my machine I'm gonna make it work I'm gonna test it locally then I'm gonna push it to get as soon as it goes to get Jenkins pulls it from as the CI and then does all the magic it runs the script there with whatever version of node on NPM they have there then they they call some other script that calls nebula that packages and publishes and then you know sometimes it happened it works sorry I like to call it Jenkins as glue because all that magic happens on Jenkins and none really knows how to reproduce it but that's the way it was and obviously it's not an ideal scenario for many reasons but I think first and foremost there was no way to manage two versions of no than NPM right and so they had version some version locally on their development machine you know and then maybe another developer would have had a different version and then on Jenkins they had something else unless someone upgraded Jenkins and then changed the version for them and then whatever ended up being packaged on the Debian the package also may be something completely different so you know there was a mismatch there that led to oftentimes two really weird problems you know people obviously complain hey this worked on my computer I don't understand why it's not working in the cloud it must be Amazon's fault right and the second thing here that's problematic is has to do with the fact that they had to use Gradle as glue right no developer is they generally speaking they're not super familiar with Gradle because they don't have to and so what ended up happening was they kind of just copy pasted you knows working scripts from other places and so we ended up with a lot of replicas of some script that someone wrote who left the company a long time ago and none really knows how it worked right it just kind of worked until it didn't and then it was really hard to fix it because no one really understood how it worked and there was a lot of lost tribal knowledge in there and then finally maybe less important is the fact that you had to have both build.gradle and the package.json and multiple types of basically build tools in your workspace it's not you know it's not super clean it's not something you'd want to have if you want to maintain that clean repo the second thing some would better second second hack is someone better not ideal either is to basically use Gradle as DUI for everything is the interface for everything okay so what they did was if they wanted to call npm install did call a Gradle task the Gradle task would call a Gradle plugin that would download NPM that would call NPM install your machine so at least that solved the problem of having separate or different versions of node on your development box and Jenkins but then you had to use Gradle to develop a node right and so I'm sure you all know that no developers are very accustomed to instant feedback and everything runs super fast Gradle does not provide that yeah and secondly we still have the problem lack of Gradle expertise right you know even if they still use Gradle as the interface it still wasn't something that he felt comfortable with and they'd left us with inconsistent solutions at best right okay and to add on top of all this we had more problems that resulted from all that hacking because we had more and more use cases as technology selection grew right we had cron jobs multiple languages for cron jobs we had Python we had Ruby we had bash we had a bunch we had lambda type works of functions that's also something that started appearing and documentation sites a completely different problems and so on and so forth not to belabor the point but really all of this made for a really really messy development environment given that everything was based on nebula alright so just to sum up the problem we we had a big lack of consistency with a repos people repeated work left and right just because they had copy/paste or didn't know what everyone else was doing there was no central guidance there was as a result there was a lot of tribal knowledge that was lost over time leading to errors and difficulty to fix things because really no one could find the person in charge we lost the telemetry which is a big thing for us as a centralized team they weren't using nebula or Gradle anymore so we kind of lost that and then obviously bad user experience if you have to run Gradle every time you run NPM it's gonna be slow no people were not happy all right time to talk about the solution so our solution just like any other good programming challenge was to use abstraction right always abstract things always add another layer abstraction it always solves things that's what I learned so first of all we realized that basically everyone no matter what platform you're in developing against everyone uses the same type of commands right you always have to initialize your project you always have to build you always have to test you have to package and publish right there was always some subtle differences like for example if you package the node project you had to have no drug time in your in your package if you pack a Java you you were dependent on Java those sort of things but generally speaking it was the same kind of verbs same type of commands except small differences okay so we came up with this model where a command is officially a repeatable implementation logic and some config on top of it and we can look at a couple of examples now that would simplify what that means so looking at the can you read this by doing okay thank you so looking at the at the package command for example this is where we take but binary artifacts and we convert them to a debian package okay this is what we later push on to the to the cloud and we bake on top of our Amazon base base machine image Amazon machine image sorry and that is what we call the immutable infrastructure this is the thing that we later use to deploy on Amazon okay so we need to package things as a debian so we have the package command we know that there are some config and we need the reusable code and as I'm sure you've already realized we have already implemented that code the nebula right we didn't run re implement this so what we ended up doing was to to make that code available by means of a container I'm sure everyone is familiar with containers right I don't need to explain containers good so effectively containers are an isolated process right it's a way for you to execute code in an environment that you know that you know exactly what it contains and how it's gonna run right and it's repeatable every time so what we did in this case was who created the container that contained the Java Runtime the AR nebula code the Gradle distribution and all the things that are needed to read from that config how to package something and then to package it so we execute the container it runs eight packages and then the container goes away and the advantage here is that the user who runs this command it doesn't even know what Randy they don't know if it's nebula or Java in fact they don't really care it's just an implementation until all they care about is that they have that config and that you know that takes care of everything okay taking a different someone different command is how we do initialization so previously before newt was a thing we asked developers to install node and to install yeoman which is a cogeneration library and to install some module and then run some node scripts to generate their code obviously that's a lot of work it there are many ways where you know this can get screwed up and so what we ended up doing was we created an initialization command that have you know its own config where you'd config what type of project you want to initialize you know what you want to call the project that sort of thing and then we'd launch the container that comes at this time with a completely different set of dependencies this time it came with node in yeoman and some glue code and the container knew how to read that config and so again we execute this it generates whatever you need to generate and then the container goes away again the user has no idea what the underlying technology is but we didn't have to reinvent anything we just kind of abstracted away all this logic within a container okay and a final example this one uses Python is the way we do documentation so one one of our efforts for us as a centralized team was to push developers to write documentation in markdown okay does anyone is a no one familiar with make Docs no just one - all right thank you so make docs is basically a Python library that allows you to take markdown code or markdown documentation and convert it to a really nice looking website the problem is that you need to have the Python tooling you need to have Python installed you need to have the Python module install you need to know what to run it's not super easy but once you have all that stuff configured you know creating beautiful looking documentation sites is really easy so we wanted to encourage that and we did this through this command effectively again that call you know the does command we would delegate to a container after mounting the current workspace to the container generate the site and then again it goes away okay so so this is where newt comes into the picture okay that's the mascot again so newt is effectively a really small lightweight go binary that we built that knows how to read a configuration file you see it there it's a done unity mo and based on that configuration files it knows based on that configuration file it knows what commands to expose to the user and what their basic configuration is so in the new EML you'd say I want my project to be called this I want my build steps to be that and then newt kind of knows how to do all the things by orchestrating the different containers or other tools it knows how to use the way we do this is basically by having a newt know how to install dependencies for you so for example if one of the commands requires docker so Newt installs docker for you if it requires NPM or nobody does that as well okay so what do we have so far we have reusable code in the form of containers we have pre-configured environments so we know every time we run those commands are always gonna run the same way right it's not gonna change it's not gonna be it's not gonna vary based on whatever they have installed on their computers we have a really simple configuration it's just a llamó file I don't know how many people here use Gradle but comparing to the groovy DSL it's you know night and day super simple it's very fast because it's built and going it works natively on all the different platforms and finally we get our telemetry back again the users are using a single tool again they're happy with the single tool we get to telemetry we can now act on it we can improve the development experience even further so you know is this good enough no because then I have to finish this presentation I have more so this is good we have we have some level of customers ability the problem is that when you have this level of customers ability you're guaranteed that people will customize it a lot more than you right people would just say oh you know I have this just one thing I have to change or you know maybe my configuration is a little bit different and so what this ends up doing is that it creates more confusion right it's just you know you get more inconsistency across the board and it's very hard for us as a centralized team to encourage you know what we consider best practices or the paved road okay so what we ended up doing to kind of fix this situation was to introduce another type of abstraction that we call app types an app type is effectively just a way for us to group a certain set of commands and defaults that makes sense and represent what we consider best practices okay so you'd have an app type for note for example that would have you know the in its build tests package deploy thing package publish and you'd have something completely different for for a docker based project right where you build an image and publish it to to a registry as I'm sure you imagine this already this is what it looks like you just specify in the new mo the app type and then you inherit all those properties from from our centrally managed definitions okay so it's not pure inheritance it's not you're not gonna have double inheritance and stuff like that but at least you get the blueprints easily and you don't have to kind of reinvent the wheel every time we get the consistency and we get to push our paper out recommendations so let me just give you two quick examples for what this looks like yeah so for node projects again we have one one sample of such node project is to use in it which again delegates to the node and yeoman container and then we can build using whatever they define for node an NPM then we can run a preview from a local container because we have docker installed so we can generate a replica of what the cloud variant would be and we do locally with tester locally we can package as Debian and we can publish again to the artifact repository the docs project I talked about previously again you initialize using some other method doesn't have to be yeoman actually you can do local preview from inside the container you build and then you publish again and there's a bunch more but I want to bury you with all the details I just want to show you a quick demo of what it looks like and this is basically the demo for the documentation site what we're going to do is we're going to edit documentation site in markdown and we're going to see what happens how the local development experience like with respect to the container is it working okay so we have a browser and two terminal windows in one terminal window we're typing nude surf what this does is it launches the container with the make docks tooling comes with Python make docks and all those things and it launches the live preview server using the the files from your laptop and then using the browser I can actually see the live preview in another terminal I'm I can edit the documentation and you see that the container was able to pick up that change and update my browser in real time and again on this computer I don't have any Python installed no make talks no anything and this is this is where it shines I think all right so with app types we can encourage best practices we simplified the tools and the processes because people don't have to install anything anymore it's just they don't have to think about what commands to to specify just comes with the app types we have some some level of customized ability but we control it right we say on the app type what are the things that we want people to customize for example the project name work where you want to install it or what do you want to call the package stuff like that and maybe most importantly is that we now centrally manage this right instead of having people kind of do their own thing and copy and paste code you know left and right we now control what that blueprint looks like and that really helps us with with pushing people for then you know doing what we consider the best practice so not everything is great Inuit land there are some things that you know we we are trying to tackle right now some things that we didn't anticipate for example some people call nude kitchen sink tool right because it's so easy you know everyone's using it it's a it's the UI for it's the interface for everything so you know naturally we're gonna put AWS credentials code in there and naturally we're gonna put you know VPN check code in there I mean why not but the problem is that you know it's growing bigger and bigger and at one point you know kitchen sink is not usually used in a as a good term so this is something we need to balance I mean how do you balance you know as a Productivity versus you know something that's bloated so that's really something we're kind of trying to figure out right now something else that we're dealing with is what I call configuration first after people have generated their project and they're happy with it and it works on the cloud you know things change and we need to upgrade libraries we need to tell them to change things it's not always easy sometimes it's a configuration thing that we can control from the app type but sometimes we really need to change code but then you have to find a person who is in charge of it if that person is still in the company you have to make sure that they deploy it and you know it doesn't crash in production and obviously you as a centralite well we as a centralized team we can't really own all of those things we can't own all of the services okay so so this is something that we that is quite a challenge for us and we're trying to to address I don't think it's unique to new it it's what people will generally call change management but because everything goes through new to is something that's kind of we're facilitating right now and then this is a really interesting problem so the way new works today is it auto-updates itself all the time right it's a self updating binary so you always have the latest version and you always have the latest version of the app types which works great for us is centralized team because we know they they are always going to have the latest version we know we're not stuck you know somewhere in the Stone Age the problem with that and you know rightfully so people are asking for reproducible builds they want to know that something they built a year ago with building exactly the same way today it's totally fair so this is something we need to balance like how do we give them that possibility but also make sure that they don't say stuck there for years on end we're you know a smallish team of three engineers we can't really support everyone and we can't really push everyone forward all the time and so this this sort of thing is something that we need to figure out perhaps I should also mention this at Netflix generally the culture is that you can tell anyone what to do and so even if even if they decide to stick with one version you know it's their call and so we need to find that balance all right so what's next for need there's a bunch of interesting things that we're going to do so first of all there's a now that we've abstracted away the commands we have the option of completely changing them right it's just an implementation detail at this point so for example the package command that I talked about leverage nebula and the verge Gradle under the hood which is ok it works ok it works pretty fast the problem is that we the nebula build is reliant on some third party that hasn't been updated in a really long time and they have really long overdue bugs that no one wants to address and so what we're what we want to do is we want to we want to change the implementation and we're doing it through a mechanism a different mechanism called debhelper which is the native debian way of packaging things and we can do this because again it's just an implementation detail so that's interesting with respect to workflows there is a bunch of stuff that we have just started thinking about which i think is really exciting stuff around source control management so you know checking out code creating branches uniformly doing PR is doing code review from the command line or from the IDE there's a bunch of so that we could do there that could really facilitate the workload for developers stuff that we haven't even begun to explore UX across the board is something that we're really passionate about and now that we have metrics we can actually address and we hope to do stuff like IDE integration I've heard that at LinkedIn they have IDE plugins that is amazing to me that's something we definitely want to look into we're thinking about doing UI for project generations so instead of using the command line to to create your projects you know just go to some UI and do all that instead of having to download things there is a bunch of stuff that we could do and then that all that change management stuff is is a really big problem that we need to address not quite sure how yet and if anyone has any solution that they've used successfully by all means please tell me I'd love to I love to hear oh okay I'll work all right that's all I have for you I don't know if you have time for questions I hope we do if not by all means feel free to talk to me if you want to talk to someone knowledgeable actually knows all the technical aspects there is Josh here I'm sorry I'm gonna embarrass you he's one of the engineers on the team please feel free to talk to him about any questions you have on a technical level we will have a question-and-answer section after all the presenters and what have you guys all come back on the stage Josh welcome to and yeah everyone can ask questions is that okay good for me awesome so thank you so much enough for introducing me talking about the problems and challenges that you guys experience and Netflix and introduction of mute and the evolution of it it's really incredible and impressive to me that you guys done this was three engineers so good job and thank you so much so next so Linda has one of the largest iOS app right large innocence was number of features in it and number of developers working on it so we have Oscar make sure it's pronounced right but Oxford Bonilla here to talk about building a scalable iOS mobile web and he's from is a staff engineer from the iOS build team thank you so as you can read in this slide I work at LinkedIn with the iOS boil build tools team and we are in charge of building the iOS app so since I only have 10 minutes I'm not gonna go into as much detail as Nadav went in the previous presentation so I'm gonna try to keep it simple and to the point so I'm gonna try and define what iOS at scale means because that's sort of an overused like if you want to make something interesting put at scale at the end right whatever at scale we're drinking beer at scale so and then I'm gonna talk about the challenges of developing at a large scale and some of the LinkedIn solutions like what we've done to cope with those challenges so what does it mean for an app to be large I'm talking so the LinkedIn app is over a million lines of code we were one of the early adopters of Swift you know we looked at Swift and says shiny and jumped on the bandwagon almost on year one so we've been dealing with it for a long long time that has impact on the build times of the app it has impact on the code how the teams communicate with each other and that sort of stuff also when you have to do transformations the Swift language at the time we adopted it was very very young and it was not clear where the language is gonna evolve to so Apple kept changing it and when you have a little app that's like a flashlight and they change the language on you it takes you like a weekend to you know change all the things that oh they made like flat map is compact map now whatever it's fine but when you have an app that has a million lines of code that gets pretty interesting on top of that the LinkedIn team is over 100 developers so it becomes complicated to decide what they're doing and to decide on the features and communicate you don't want to be rewriting how you do some stuff while the other team is rewriting the network stack and then you don't need to put them back together we also have a large number of commits per day because we have that main people and then what ends up happening is that things that only happen 1% of the time or half a percent of the time happen all the time and we have to deal with them so those are sort of the challenges so I'll tell you some war stories later on and then we also have to deal with testing and nobody ever deletes a test they don't they're strictly additive so we have our thousand UI tests and if you just ran them like Apple wants you to run them like you click paste the next code it will take over 13 hours so that's no good so we had to deal with long test times as well so that's sort of the context for what a large app means so what are the challenges so I tried to divide them in three broad categories one is people challenges because you know so many developers working on the same on the same thing need to organize somehow the second one is infrastructure challenges because at LinkedIn we run our own data centers we have our own hardware so we have to deal with the machines themselves and iOS apps can only be built on Apple hardware so that makes the problem if you have a large data center that has thousands of machines that is complicated in and of itself if those machines are Apple machines that were designed for a consumer market and not for data centers that makes it even more interesting so there's a bunch of challenges there and also the software all the tooling all the you know how do you build it how do you package it the compilers the linkers the AC tool thing that I be tooling the assets the simulator cycle go on and on but I want we don't have enough time so the people basically you can imagine what the problems are that these are the normal problems of large teams how do you communicate among the teams how do you share documentation how do you do the code maintenance that picture is actually a real picture of LinkedIn developers dealing with our Swift 3 migration party which was basically take the million lines of code from Swift to make him be Swift 3 and Xcode very helpfully provided a tool that was supposed to automatically transform your source code and it crashed because we had millions of lines of code and it wasn't expecting that much and we strive for maintaining the feel of single authors so I mean that's that's sort of the ideal you'd want to look at the code and imagine that a small team of two maybe three people wrote the whole thing in a weekend and it has self-consistency but we have a hundred or more developers so so maintaining that that single author feel across such a large app becomes very very challenging infrastructure wise that's a picker for one of our data centers and mainly the problems revolve around I mean it's all the normal problems that that run in a large data center and usually involves plus machines not really the sign for data center use the standard Mac for a data center is the Mac Mini because it's the only computer that Apple makes that doesn't have display built into the computer and people have done crazy things like taking IMAX and you know rackmount him on the side and all sorts but we stuck with the Mac minis and even those have very interesting challenges like you basically have to hit command R for this operation and like how do I hit command R on a thousand computers that don't have a keyboard attached so we have to come up with some creative solutions and I'll talk a little bit about those as far as I know there is no any kind of lights-out management for Mac Mini so we had to deal with that as well and software upgrades are really really fun to do across thousands of machines you can easily imagine that and then we have all the tooling right like you know you we have this weird thing where if you have a generic in swift and you look at it from an objective-c thing whatever the thing crashes and that only happens half a percent of the time and for us it happens all the time and we have these we have this really fun fun bug where you have basically 1% of the bills were failing because own was saying the user doesn't exist but that you own right before it work with the same user and we ended up finding that it was a combination of an invalid group that crashes open directory D and then if you do it your own while it's crash before it restarts that little window then it fails right so it's it's one of those that you could never imagine until you see it also producing matrix is a problem because when you have such a large I mean all of the machines are in different states and you know you run a build and this machine happens to have been running something else like you know you know you need to know to disable spot like for instance because otherwise it goes and touches all of your files and just finding the right combination and and making sure we understand what's going on in a in a global view not individual machines requires having very precise metrics so that is hard reporting becomes a problem as well because you can have different kinds of failures when you're running a CI system you can have failures that are due to the team's problems within the code that the team owns like you know they make a mistake they type something that that doesn't work that shouldn't work and then you want to communicate back what they did and how to fix it all the times you're having infrastructure failures like you know they hit a machine that happen to have a busted version of Xcode installed and then you want to communicate that clearly says somebody can take a look at it and in the millions of lines of logs that you end up it's not a good solution to go back and say here's the log you find it right because nobody likes looking at those things to lien upgrades become very very problematic speed of builds as well we have been dealing with slow build space and trying to speed that up and I'll talk a little bit more about it and also lots and lots of tests and we we have had a lot of trouble dealing with flaky tests and I'll show some solutions so I'm gonna go back to the three broad themes and just talk about some of the LinkedIn solutions that we have so the first one for people is a lot of communication our philosophy is you can't over communicate so basically keep everyone involved keep everyone in the loop talk about what you're doing show the projects have shared the metrics sure the state of the projects basically have a culture of complete openness and transparency so that everyone knows what's going on because you know even after you told everyone what you're doing and you're blue in the face of saying the same thing somebody will show up and you're like I didn't know you were doing that thing like this took me completely by surprise even after you know tens and tens of emails daily saying I'm gonna do this yeah I'm gonna brush it up a lil own infrastructure the infrastructure side basically the solution is automate everything nothing needs to be I mean if any step is manual then it absolutely would won't work so basically we treat the infrastructure as code it's a product that it gets built tested and deployed in the same way as all the other products we have for instance one one piece of code that's called OVH see the onboard health checks and it's basically a daemon that keeps running we built this thing it keeps writing the Machine and it checks for all sorts of things and you can it's like a plugin based system where you can add a bunch of checks and it'll go and make sure that you have the right version of Xcode the machine has a right version of Mac OS that spotlight is disabled that all of this preconditions to it being a healthy machine or let and then it can check this before a job after a job and basically manage the whole thing and that in and of itself is also deployed through the same system that we deploy everything else so it goes through code review and we have a standardized system for doing that and then for tooling a lot of rappers almost everything gets wrapped around things where we will do things like make sure that if it fails it gets retried because sometimes there are some cases where just refrying works but then we have to make sure to add it to the metrics that this thing got retried so because we tried too many times then we need to go look at it we have a unified system for metrics that is for across the company so we plug into that thing happy Bill failures is this broad initiative we're trying to actually make sense of the error messages so that they're actionable and people can look at them and go oh I know what's going on right then the process around upgrading that's we basically found that having multiple versions of X coding still on the same machine just some work as well and I talked to the expert team recently and they fix this to a certain degree but we just went single X code you know it's like single malt you don't want to mix your molds so we went with the single X cube per machine solution and that has worked pretty well for us to the point that we don't want to go back even if Apple fixed it then we're also doing some we're switching the build system this is kind of you know change the engine while the car is running we're moving from Mexico to Basel because it basically biases bill caching Basel is a build system that was it's like an open-source version of Google's Blaze system and it has this property where it can run commands in a hermetic fashion and if it if they're fully determined by the inputs and the outputs you can actually put a bill cache and for a bunch of stuff you don't need to actually build it you can go a fetch it from the cache so you make the bills really fast and the the screenshot that you're looking at is another tool that we built it's called blue pill and what it does is that it allows you to run multiple simulators in parallel and we run a bunch of simulators in parallel we actually every execution takes over four machines and every machine runs five simulators in parallel and the only limit that we have on how many we can run is a number of course in the machines now that Apple has released new Mac minis that have six process six processors that stuff course then we will run 12 of these simulators in parallel so we're gonna make our tests faster that way and this tool is actually pretty nice because it also deals with flakiness in the test so it basically will retry the tests if they fail it reports it if they hang it gets a stack trace and kills it and if they crash it gets a stuck trace and moves on basically and it also deals with the infrastructure failures so if the simulator is failed to boot then it kills them restarts them in price again reports it and it's all happiness so far and that's about it and if you have any questions or want to know more about what we're doing come talk to me find me I'll be around well thank you so much Oscar so what you know I was working on product infrastructure one this one person mysterious failure was happening so now I know why so Thank You Oscar for explaining that I'm also a very big fan of happy Bill failure when I was doing development nothing bothered me more than an ambiguous error message and I don't know what to do this so thank you so much next up we have Nima Denis Nima started here I LinkedIn as a intern this is actually his first job now he's a senior engineer he got his master's from UT Austin and working on his PhD also at UT Austin Nia doesn't know this yet but I also am a UT Austin graduate so go Longhorns so he's here to talk about uniform product development experience here at LinkedIn he works on the NP multi product team welcome me good evening this is going to be the last talk hope you are still awake I'm Nima I work at the multi product team and my talk is going to be about multi products I want to tell you what they are how they enable engineers at LinkedIn to be more productive and how they assist us with automation and continuous integration at any software Enterprise one fundamental question is how would you organize your code base would you rather have a single giant repository that holds the code base for all of your products which is called the mana repo and companies like Google do it this way or would you have a separate repository for each product which is called a multi repo and that's what we do at LinkedIn and hence we created the concept of a multi product so multi product is a separate business entity that is developed tested build and deploy it independently any multi product is tied to a version control system git or subversion those are the two that we commonly use here a multi product is also tied to a build system like Gradle or basil SBT and so and so forth and is I tried to a deployment back-end if it's a deployable like the standard LinkedIn deployment which we call it or other deployment techniques like Hadoop or Sansa etc so essentially multi-product is an abstraction just like you saw in the first talk newt so there is also a product spec for a multi product that users can specify how should their product look like what are the dependencies like it depends on what products and how the bill should look like how the test should look like and all of those details so we have a command-line tool named mink or multi product intelligence so mint is a command-line tool that allows you to perform common operations on multi products for instance if you want to create a new multi product you can open a terminal and type mint product create and that would prompt you for the product name the type of the source control you want to use whether it's git or SVN it asks you for the product templates we have a predefined set of templates and you can also create a template so everyone at thinking can use and you cannot you should also specify the owners and the product creation automatically happens for you it creates a repository it assigns it to the multi product and it creates like the skeleton for the build system you are using if it's Gradle for instance and now say you want to clone the codebase of someone else's multi product on your machine you can use mint check out that product and all you need to know is that product name you don't need to know if that is on there kids or subversion you don't need to know where it is hosted like what is the URL you just type this command and you have a clone of that product you can start working on it and experimenting with it and the next step is if you want to build that product you just cloned locally you just type mint build again you don't need to know the implementation details because of abstraction or whether what's the build system what is the build command that's specified in products back and if your product is a deployable you can locally deploy it by running mint deploy and under the hood it knows how it should be deployed so this Universal abstraction this universal interface mint has several benefits for developer productivity anyone at LinkedIn can clone any product and start working on it very easy and anyone that owns the product can specify their own custom commands' in the products back and mean would honor that you can execute it and you can glue a bunch of your commands together and create new commands and run them and the benefit of this abstraction for automation and continuous delivery is I'm going to talk about it now but before let's look at the development lifecycle at Lincoln so first you write code then you create a review to ask for peer reviews and at this point you want to check in your code so before your code is checked in there is a pretty committed CI step that essentially does some lightweight validation on your code some style checking some version checks and all those and after that your code is checked into the remote repo and we would trigger a postman job which is a more heavyweight validation and at the end it will publish a version of your new version of your product so let's look at how this abstraction helps in defining what the post commit step should look like with this mint abstraction a post commit step is nothing more than a chain of mint commands because you need to add at this point your product is checked into the remote repository you need to check out the product you need to build it you need to run the test deploy it and at the end you need to publish the new version to a common place named artifactory that anyone can use and now let's look at one command that is very popular among developers here is called mint WC test or working copy test after you done your local testing and development sometimes you want to test your change in a remote environment to make sure there is no system dependency that like some environment you have on your machine but it's not present in a standard environment and this command allows you to do that so the way it works is imagine you are working on product key you make your change and now you want to run you don't meet the VC test in a terminal under the hood mint knows what is your source control is it git or subversion and based on that implementer it creates a patch like a div that contains your change it uploads it somewhere to the cloud and then it kicks off a validation job which is again a chain of main commands you check out the product at the revision the developer was working on locally you apply the DIF that you uploaded and then you can build it and do other checks so one of the checks that we do here which is we find very strong and valuable is called compatibility testing and that also happens as part of a WC test run so let's talk about compatibility testing so multi-product compatibility testing wants to essentially test that it changes backwards compatible with respect to all the consumers here we enforce semantic versioning it's a very standard schema and any developer that has a product should honor this contract so the contract says your versions are consists of three parts major minor and patch and whenever you want to make a backwards incompatible change you have to increment your major version whenever you introduce some new functionality in a backwards compatible way you bump your minor version and whenever you make some bug fixes you bump your patch version for example if you have this example that product C at version 1 to 3 depends on product P let's say you make a change to product P and it's a small change it's a bug fix you go to version 1 0 1 and assuming that previously product C the build and test lifecycle passed meaning that product C was happy with product P after this change it's still the same test should pass that means the change to people's backwards-compatible and the way to enforce this in multi product framework in the same devil you see this example that we saw is because of that universal contract were versioning we are able to find all of your consumers that depend on the same major version of your product and run their build and test lifecycle parallel and of course this is a very expensive step because for some mainstream products they have thousands of consumers but it's a very valuable signal that if you receive that signal you're confident that your change would not suddenly start breaking some of your consumers that's all I had it was a very introductory talk and find me after there is there's going to be a Q&A now and we can chat after Thanks</p>